{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13a3afd",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter tuning and Error Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572d804",
   "metadata": {},
   "source": [
    "## 3.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a435e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries, data and clean3 function from Day-2\n",
    "\n",
    "## Libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Load train / validation data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "  train,\n",
    "  test_size=0.20,\n",
    "  stratify=train['target'],\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "## Define clean3\n",
    "def clean3(text):\n",
    "  text = text.lower() # lowercasing\n",
    "  text = re.sub(r\"#([a-z0-9_]+)\", r\"\\1\", text) # Hashtag to plain word\n",
    "  text = re.sub(r'http\\S+', \"\", text) # removing HTTP. URL\n",
    "  text = re.sub(r\"www\\.\\S+\", \"\", text) # removing WWW. URL\n",
    "  text = re.sub(r'@\\w+', \"\", text) # removing @mentions\n",
    "  text = re.sub(r\"[^a-z0-9\\s]\", \" \", text) #r emoving other characters other than a-z, 0-9 and whitespace\n",
    "  text = re.sub(r\"\\s+\", \" \", text).strip() # Changing multiple spaces into one\n",
    "  return text\n",
    "\n",
    "##Apply clean3 \n",
    "train_df['clean3'] = train_df['text'].apply(clean3)\n",
    "val_df['clean3']   = val_df['text'].apply(clean3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10aed1b",
   "metadata": {},
   "source": [
    "### 3.1.1 Define `eval_params`\n",
    "\n",
    "This function trains a TF-IDF → Logistic Regression pipeline on **`train_df['clean3']`** and evaluates on **`val_df['clean3']`**. \n",
    "It returns the validation F1 score for the given `C`, `ngram_range`, and `min_df` parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab3f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_params(clean_col, C, ngram_range, min_df):\n",
    "\n",
    "  # 1. Vectorize\n",
    "  vect = TfidfVectorizer(ngram_range = ngram_range, min_df = min_df)\n",
    "  X_tr = vect.fit_transform(train_df[clean_col])\n",
    "  y_tr = train_df['target']\n",
    "\n",
    "  X_vl = vect.transform(val_df[clean_col])\n",
    "  y_vl = val_df['target']\n",
    "\n",
    "  # 2. Train Logistic Regression\n",
    "  lr = LogisticRegression(C=C, max_iter=1000)\n",
    "  lr.fit(X_tr, y_tr)\n",
    "\n",
    "  # 3. Predict & compute F1\n",
    "  preds = lr.predict(X_vl)\n",
    "  return f1_score(y_vl, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db636f0",
   "metadata": {},
   "source": [
    "### 3.1.2 Grid‐Search Loop\n",
    "\n",
    "We will iterate over all combinations of C, ngram_range, and min_df. \n",
    "For each combination, we call `eval_params('clean3', C, ngram_range, min_df)` \n",
    "and track the best validation F1 and its corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92fc9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation F1: 0.7777777777777778\n",
      "Best params: C = 10 , ngram_range = (1, 3) , min_df = 2\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.01, 0.1, 1, 10]\n",
    "ngram_ranges = [(1, 2), (1, 3), (2, 3), (3, 5)]\n",
    "min_dfs = [1, 2, 5]\n",
    "\n",
    "best_score = 0.0\n",
    "best_params = None\n",
    "\n",
    "for C in Cs:\n",
    "  for ngram in ngram_ranges:\n",
    "    for md in min_dfs:\n",
    "      score = eval_params('clean3', C=C, ngram_range=ngram, min_df=md)\n",
    "      if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = (C, ngram, md)\n",
    "\n",
    "print(\"Best validation F1:\", best_score)\n",
    "print(\"Best params: C =\", best_params[0],\n",
    "      \", ngram_range =\", best_params[1],\n",
    "      \", min_df =\", best_params[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee10f0",
   "metadata": {},
   "source": [
    "## 3.2 Error Analysis\n",
    "### 3.2.1 Re-fit on train_df with Best Hyperparameters\n",
    "\n",
    "We will use:\n",
    "- C = 10\n",
    "- ngram_range = (1, 3)\n",
    "- min_df = 2\n",
    "\n",
    "Then we’ll obtain predicted probabilities on **val_df['clean3']** (so we can measure how “sure” the model was about each prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160b1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "C_best, ngram_best, min_df_best = 10, (1, 3), 2\n",
    "\n",
    "# Fit TF-IDF on train_df['clean3']\n",
    "vect_best = TfidfVectorizer(ngram_range=ngram_best, min_df=min_df_best)\n",
    "x_tr_best = vect_best.fit_transform(train_df['clean3'])\n",
    "y_tr_best = train_df['target']\n",
    "\n",
    "# Appluing Logistic regression with C=10\n",
    "lr_best = LogisticRegression(C=C_best, max_iter=1000)\n",
    "lr_best.fit(x_tr_best, y_tr_best)\n",
    "\n",
    "# Transform val_df['clean3'] → get probabilities & predictions\n",
    "X_vl_best = vect_best.transform(val_df['clean3'])\n",
    "val_probs = lr_best.predict_proba(X_vl_best)[:, 1] \n",
    "val_preds = lr_best.predict(X_vl_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48802958",
   "metadata": {},
   "source": [
    "### 3.2.2 Compute Confidence Error and find top 20\n",
    "\n",
    "For each validation tweet:\n",
    "- If true label=1 ⇒ error_conf = (1 − probability).  \n",
    "- If true label=0 ⇒ error_conf = (probability).  \n",
    "\n",
    "We sort descending by error_conf and pick the top 20 tweets the model was most “confident yet wrong” on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135d98c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean3</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "      <th>error_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>You can never escape me. Bullets don't harm me...</td>\n",
       "      <td>you can never escape me bullets don t harm me ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.996779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5435</th>\n",
       "      <td>Maid charged with stealing Dh30000 from police...</td>\n",
       "      <td>maid charged with stealing dh30000 from police...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987849</td>\n",
       "      <td>0.987849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>if firefighters acted like cops they'd drive a...</td>\n",
       "      <td>if firefighters acted like cops they d drive a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983526</td>\n",
       "      <td>0.983526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>Do you feel like you are sinking in low self-i...</td>\n",
       "      <td>do you feel like you are sinking in low self i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017163</td>\n",
       "      <td>0.982837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>I can't drown my demons they know how to swim</td>\n",
       "      <td>i can t drown my demons they know how to swim</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018760</td>\n",
       "      <td>0.981240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>Chick masturbates a guy until she gets explode...</td>\n",
       "      <td>chick masturbates a guy until she gets explode...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022208</td>\n",
       "      <td>0.977792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>Keep shape your shoes ??#Amazon #foot #adjust ...</td>\n",
       "      <td>keep shape your shoes amazon foot adjust shape...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Bloody insomnia again! Grrrr!! #Insomnia</td>\n",
       "      <td>bloody insomnia again grrrr insomnia</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024493</td>\n",
       "      <td>0.975507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6212</th>\n",
       "      <td>@PianoHands You don't know because you don't s...</td>\n",
       "      <td>you don t know because you don t smoke the way...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025282</td>\n",
       "      <td>0.974718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>@SatanaOfHell ever seen by far. A dreamy look ...</td>\n",
       "      <td>ever seen by far a dreamy look came over his f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026304</td>\n",
       "      <td>0.973696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>I just watched emmerdale nd I don't know who m...</td>\n",
       "      <td>i just watched emmerdale nd i don t know who m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028375</td>\n",
       "      <td>0.971625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>Why are you deluged with low self-image? Take ...</td>\n",
       "      <td>why are you deluged with low self image take t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029734</td>\n",
       "      <td>0.970266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>My prediction for the Vikings game this Sunday...</td>\n",
       "      <td>my prediction for the vikings game this sunday...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030817</td>\n",
       "      <td>0.969183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>Does the #FingerRockFire make you wonder 'am I...</td>\n",
       "      <td>does the fingerrockfire make you wonder am i p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031133</td>\n",
       "      <td>0.968867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7552</th>\n",
       "      <td>Israel wrecked my home. Now it wants my land. ...</td>\n",
       "      <td>israel wrecked my home now it wants my land</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032643</td>\n",
       "      <td>0.967357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>Chernobyl disaster - Wikipedia the free encycl...</td>\n",
       "      <td>chernobyl disaster wikipedia the free encyclop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034745</td>\n",
       "      <td>0.965255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6488</th>\n",
       "      <td>Aquarium Ornament Wreck Sailing Boat Sunk Ship...</td>\n",
       "      <td>aquarium ornament wreck sailing boat sunk ship...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035180</td>\n",
       "      <td>0.964820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>Aquarium Ornament Wreck Sailing Boat Sunk Ship...</td>\n",
       "      <td>aquarium ornament wreck sailing boat sunk ship...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035180</td>\n",
       "      <td>0.964820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>@UnrealTouch fuck sake john Jesus my heart jus...</td>\n",
       "      <td>fuck sake john jesus my heart just sunk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>0.964004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>Fascinating pics from inside North Korea. Not ...</td>\n",
       "      <td>fascinating pics from inside north korea not p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963832</td>\n",
       "      <td>0.963832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "4154  You can never escape me. Bullets don't harm me...   \n",
       "5435  Maid charged with stealing Dh30000 from police...   \n",
       "1358  if firefighters acted like cops they'd drive a...   \n",
       "6108  Do you feel like you are sinking in low self-i...   \n",
       "2905      I can't drown my demons they know how to swim   \n",
       "3435  Chick masturbates a guy until she gets explode...   \n",
       "6317  Keep shape your shoes ??#Amazon #foot #adjust ...   \n",
       "895            Bloody insomnia again! Grrrr!! #Insomnia   \n",
       "6212  @PianoHands You don't know because you don't s...   \n",
       "3909  @SatanaOfHell ever seen by far. A dreamy look ...   \n",
       "2068  I just watched emmerdale nd I don't know who m...   \n",
       "2250  Why are you deluged with low self-image? Take ...   \n",
       "4609  My prediction for the Vikings game this Sunday...   \n",
       "7325  Does the #FingerRockFire make you wonder 'am I...   \n",
       "7552  Israel wrecked my home. Now it wants my land. ...   \n",
       "5114  Chernobyl disaster - Wikipedia the free encycl...   \n",
       "6488  Aquarium Ornament Wreck Sailing Boat Sunk Ship...   \n",
       "6465  Aquarium Ornament Wreck Sailing Boat Sunk Ship...   \n",
       "6494  @UnrealTouch fuck sake john Jesus my heart jus...   \n",
       "2773  Fascinating pics from inside North Korea. Not ...   \n",
       "\n",
       "                                                 clean3  target  pred  \\\n",
       "4154  you can never escape me bullets don t harm me ...       1     0   \n",
       "5435  maid charged with stealing dh30000 from police...       0     1   \n",
       "1358  if firefighters acted like cops they d drive a...       0     1   \n",
       "6108  do you feel like you are sinking in low self i...       1     0   \n",
       "2905      i can t drown my demons they know how to swim       1     0   \n",
       "3435  chick masturbates a guy until she gets explode...       1     0   \n",
       "6317  keep shape your shoes amazon foot adjust shape...       1     0   \n",
       "895                bloody insomnia again grrrr insomnia       1     0   \n",
       "6212  you don t know because you don t smoke the way...       1     0   \n",
       "3909  ever seen by far a dreamy look came over his f...       1     0   \n",
       "2068  i just watched emmerdale nd i don t know who m...       1     0   \n",
       "2250  why are you deluged with low self image take t...       1     0   \n",
       "4609  my prediction for the vikings game this sunday...       1     0   \n",
       "7325  does the fingerrockfire make you wonder am i p...       1     0   \n",
       "7552        israel wrecked my home now it wants my land       1     0   \n",
       "5114  chernobyl disaster wikipedia the free encyclop...       1     0   \n",
       "6488  aquarium ornament wreck sailing boat sunk ship...       1     0   \n",
       "6465  aquarium ornament wreck sailing boat sunk ship...       1     0   \n",
       "6494            fuck sake john jesus my heart just sunk       1     0   \n",
       "2773  fascinating pics from inside north korea not p...       0     1   \n",
       "\n",
       "          prob  error_conf  \n",
       "4154  0.003221    0.996779  \n",
       "5435  0.987849    0.987849  \n",
       "1358  0.983526    0.983526  \n",
       "6108  0.017163    0.982837  \n",
       "2905  0.018760    0.981240  \n",
       "3435  0.022208    0.977792  \n",
       "6317  0.024000    0.976000  \n",
       "895   0.024493    0.975507  \n",
       "6212  0.025282    0.974718  \n",
       "3909  0.026304    0.973696  \n",
       "2068  0.028375    0.971625  \n",
       "2250  0.029734    0.970266  \n",
       "4609  0.030817    0.969183  \n",
       "7325  0.031133    0.968867  \n",
       "7552  0.032643    0.967357  \n",
       "5114  0.034745    0.965255  \n",
       "6488  0.035180    0.964820  \n",
       "6465  0.035180    0.964820  \n",
       "6494  0.035996    0.964004  \n",
       "2773  0.963832    0.963832  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vl = val_df['target'].values\n",
    "\n",
    "# Computing confidence error\n",
    "error_conf = np.where(y_vl == 1, 1 - val_probs, val_probs)\n",
    "\n",
    "# Attach to a copy of val_df\n",
    "val_df_copy = val_df.copy()\n",
    "val_df_copy['prob'] = val_probs\n",
    "val_df_copy['pred'] = val_preds\n",
    "val_df_copy['error_conf'] = error_conf\n",
    "\n",
    "# Sort by error_conf descending and take top 20\n",
    "most_wrong = val_df_copy.sort_values(by='error_conf', ascending=False).head(20)\n",
    "\n",
    "# Display relevant columns\n",
    "most_wrong[['text', 'clean3', 'target', 'pred', 'prob', 'error_conf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15650c",
   "metadata": {},
   "source": [
    "### 3.2.3 Inspect Top 20 Mistakes\n",
    "\n",
    "Above are the 20 validation tweets our tuned model was most confident about but still misclassified. \n",
    "Each row suggests:\n",
    "- `text` (raw tweet)\n",
    "- `clean3` (preprocessed text)\n",
    "- `target` (true label)\n",
    "- `pred` (model’s predicted label)\n",
    "- `prob` (predicted P(class=1))\n",
    "- `error_conf` (how confidently wrong the model was)\n",
    "\n",
    "Above Table shows that there are three common error types; Missing content, Ambiguous language and Bigram / Trigrams. \n",
    "Thus, we will make a short list of disaster words, in order to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f19e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + has_kw Val F1: 0.7762128325508607\n",
      "TF-IDF + has_kw + has_loc Val F1: 0.7756059421422987\n",
      "{'TF-IDF only': 0.7777777777777778, 'TF-IDF + has_kw': 0.7762128325508607, 'TF-IDF + has_kw + has_loc': 0.7756059421422987}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "disaster_words = [\n",
    "    'earthquake', 'flood', 'hurricane', 'wildfire', 'tornado',\n",
    "    'tsunami', 'blizzard', 'volcano', 'landslide', 'avalanche'\n",
    "]\n",
    "\n",
    "places = [\n",
    "    'california', 'texas', 'new york', 'florida', 'london', 'paris',\n",
    "    'india', 'japan', 'china', 'germany', 'france'\n",
    "]\n",
    "\n",
    "train_df['has_kw'] = train_df['clean3'].apply(lambda t: int(any(w in t.split() for w in disaster_words)))\n",
    "val_df['has_kw']   = val_df['clean3'].apply(lambda t: int(any(w in t.split() for w in disaster_words)))\n",
    "\n",
    "train_df['has_loc'] = train_df['text'].apply(lambda t: int(any(p in t.lower().split() for p in places)))\n",
    "val_df['has_loc']   = val_df['text'].apply(lambda t: int(any(p in t.lower().split() for p in places)))\n",
    "\n",
    "X_tr_text = vect_best.transform(train_df['clean3'])\n",
    "X_vl_text = vect_best.transform(val_df['clean3'])\n",
    "\n",
    "X_tr_kwflag = csr_matrix(train_df['has_kw'].values.reshape(-1, 1))\n",
    "X_vl_kwflag = csr_matrix(val_df['has_kw'].values.reshape(-1, 1))\n",
    "\n",
    "X_tr_locflag = csr_matrix(train_df['has_loc'].values.reshape(-1, 1))\n",
    "X_vl_locflag = csr_matrix(val_df['has_loc'].values.reshape(-1, 1))\n",
    "\n",
    "X_tr_combo = hstack([X_tr_text, X_tr_kwflag])\n",
    "X_vl_combo = hstack([X_vl_text, X_vl_kwflag])\n",
    "\n",
    "lr_kw = LogisticRegression(C=C_best, max_iter=1000)\n",
    "lr_kw.fit(X_tr_combo, train_df['target'])\n",
    "preds_kw = lr_kw.predict(X_vl_combo)\n",
    "print(\"TF-IDF + has_kw Val F1:\", f1_score(val_df['target'], preds_kw))\n",
    "\n",
    "X_tr_combo2 = hstack([X_tr_text, X_tr_kwflag, X_tr_locflag])\n",
    "X_vl_combo2 = hstack([X_vl_text, X_vl_kwflag, X_vl_locflag])\n",
    "\n",
    "lr_kwloc = LogisticRegression(C=C_best, max_iter=1000)\n",
    "lr_kwloc.fit(X_tr_combo2, train_df['target'])\n",
    "preds_kwloc = lr_kwloc.predict(X_vl_combo2)\n",
    "print(\"TF-IDF + has_kw + has_loc Val F1:\", f1_score(val_df['target'], preds_kwloc))\n",
    "\n",
    "results_flags = {\n",
    "    \"TF-IDF only\": f1_score(val_df['target'], lr_best.predict(X_vl_best)),\n",
    "    \"TF-IDF + has_kw\": f1_score(val_df['target'], preds_kw),\n",
    "    \"TF-IDF + has_kw + has_loc\": f1_score(val_df['target'], preds_kwloc)\n",
    "}\n",
    "print(results_flags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0958034",
   "metadata": {},
   "source": [
    "Now with the 'TF-IDF only' method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb212e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1) Load test set and apply clean3\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['clean3'] = test['text'].apply(clean3)\n",
    "\n",
    "# 2) Fit TF-IDF (1–3 grams, min_df=2) on full train_df['clean3']\n",
    "vect_full = TfidfVectorizer(ngram_range=(1, 3), min_df=2)\n",
    "X_full = vect_full.fit_transform(train_df['clean3'])\n",
    "y_full = train_df['target']\n",
    "\n",
    "# 3) Train Logistic Regression with C=10\n",
    "lr_full = LogisticRegression(C=10, max_iter=1000)\n",
    "lr_full.fit(X_full, y_full)\n",
    "\n",
    "# 4) Transform test['clean3'] and predict\n",
    "X_test = vect_full.transform(test['clean3'])\n",
    "test_preds = lr_full.predict(X_test)\n",
    "\n",
    "# 5) Build submission and save\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd2081",
   "metadata": {},
   "source": [
    "submission result was lower than before: 0.79037.\n",
    "\n",
    "**Takeaway:**  \n",
    "- All three “extra feature” trials lowered F1.  \n",
    "- This tells us our baseline TF-IDF + LogReg is already capturing most signal—adding noisy keyword flags or char ngrams without further filtering did more harm than good.  \n",
    "- Next, we’ll accept the baseline hyperparameters and move to 5-fold CV and blending."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
