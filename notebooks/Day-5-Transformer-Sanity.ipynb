{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe1340c",
   "metadata": {},
   "source": [
    "# 5. Transformer Sanity Check with DistilBERT\n",
    "**Objective:** Get a complete, end-to-end Transformer fine-tuning pipeline working. The goal is not a high score, but to ensure all new libraries and components are set up correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041962c9",
   "metadata": {},
   "source": [
    "## 5.1 Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0045c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
    "BATCH_SIZE = 16 # You can lower this if you run out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608818c",
   "metadata": {},
   "source": [
    "## 5.2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940f5524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (6851, 5)\n",
      "Validation set shape: (762, 5)\n",
      "\n",
      "Columns are now: ['id', 'keyword', 'location', 'text', 'label', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "# Load the Full training data\n",
    "full_train_df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# Define clean3\n",
    "def clean3(text):\n",
    "  text = text.lower() # lowercasing\n",
    "  text = re.sub(r\"#([a-z0-9_]+)\", r\"\\1\", text) # Hashtag to plain word\n",
    "  text = re.sub(r'http\\S+', \"\", text) # removing HTTP. URL\n",
    "  text = re.sub(r\"www\\.\\S+\", \"\", text) # removing WWW. URL\n",
    "  text = re.sub(r'@\\w+', \"\", text) # removing @mentions\n",
    "  text = re.sub(r\"[^a-z0-9\\s]\", \" \", text) #removing other characters other than a-z, 0-9 and whitespace\n",
    "  text = re.sub(r\"\\s+\", \" \", text).strip() # Changing multiple spaces into one\n",
    "  return text\n",
    "\n",
    "full_train_df['text'] = full_train_df['text'].apply(clean3)\n",
    "\n",
    "# Changing the name to adhere to the default naming convention\n",
    "full_train_df = full_train_df.rename(columns={'target': 'label'})\n",
    "\n",
    "# Creating a 90/10 split\n",
    "train_df, val_df = train_test_split(\n",
    "  full_train_df,\n",
    "  test_size=0.1,\n",
    "  stratify= full_train_df['label'],\n",
    "  random_state=42\n",
    ") \n",
    "\n",
    "# Convert pandas dataframe to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"\\nColumns are now:\", train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78eb39",
   "metadata": {},
   "source": [
    "## 5.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4898f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef39fac465ab4b0fbe5d43355a13c436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4919041c564144788afc8cb17f2e91a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/762 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Creating a function to tokenize text\n",
    "def tokenize_function(examples):\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# Applying the tokenization to dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched= True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched= True)\n",
    "\n",
    "# Enable dynamic padding \n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e510d",
   "metadata": {},
   "source": [
    "## 5.4. Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4788b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 12:28:22.302517: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749720503.066957  103513 meta_optimizer.cc:967] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 240s 525ms/step - loss: 0.4328 - accuracy: 0.8145 - val_loss: 0.4114 - val_accuracy: 0.8189\n",
      "Epoch 2/3\n",
      "429/429 [==============================] - 205s 478ms/step - loss: 0.3058 - accuracy: 0.8774 - val_loss: 0.4234 - val_accuracy: 0.8346\n",
      "Epoch 3/3\n",
      "429/429 [==============================] - 209s 486ms/step - loss: 0.1971 - accuracy: 0.9277 - val_loss: 0.5378 - val_accuracy: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x3521cf110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the pre-trained model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
    "\n",
    "# Preparing datasets for Tensorflow\n",
    "tf_train_dataset = tokenized_train_dataset.to_tf_dataset(\n",
    "  columns = [\"attention_mask\", \"input_ids\", \"label\"],\n",
    "  shuffle = True,\n",
    "  batch_size = BATCH_SIZE,\n",
    "  collate_fn = data_collator,\n",
    ")\n",
    "\n",
    "tf_val_dataset = tokenized_val_dataset.to_tf_dataset(\n",
    "  columns = [\"attention_mask\", \"input_ids\", \"label\"],\n",
    "  shuffle = False,\n",
    "  batch_size =BATCH_SIZE,\n",
    "  collate_fn = data_collator,\n",
    ")\n",
    "\n",
    "# Compile and Train\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Fine tuning\n",
    "model.fit(tf_train_dataset, validation_data=tf_val_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a99613",
   "metadata": {},
   "source": [
    "## 5.5. Sanity Check Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c786a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 13s 255ms/step\n",
      "\n",
      "Sanity Check Validation F1 Score: 0.79228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Get predictions for the validation set\n",
    "val_logits = model.predict(tf_val_dataset).logits\n",
    "val_probs = tf.nn.softmax(val_logits, axis=1).numpy()\n",
    "val_preds = np.argmax(val_probs, axis=1)\n",
    "\n",
    "# Getting true labels\n",
    "y_true = val_df['label'].to_numpy()\n",
    "\n",
    "# Calculating F1 score\n",
    "f1 = f1_score(y_true, val_preds)\n",
    "\n",
    "print(f\"\\nSanity Check Validation F1 Score: {f1:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
